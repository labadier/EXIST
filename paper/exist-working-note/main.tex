%% The first command in your LaTeX source must be the \documentclass command.
%%
%% Options:
%% twocolumn : Two column layout.
%% hf: enable header and footer.
\documentclass[
% twocolumn,
% hf,
]{ceurart}

%%
%% One can fix some overfulls
\sloppy

%%
%% Minted listings support 
%% Need pygment <http://pygments.org/> <http://pypi.python.org/pypi/Pygments>
\usepackage{listings}
%% auto break lines
\lstset{breaklines=true}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% Rights management information.
%% CC-BY is default license.
\copyrightyear{2022}
\copyrightclause{Copyright for this paper by its authors.
  Use permitted under Creative Commons License Attribution 4.0
  International (CC BY 4.0). }

%%
%% This command is for the conference information
\conference{IberLEF 2022, Spetember 2022, A Coruña, Spain.}

%%
%% The "title" command
\title{Is Data Worth More Than Language?}

\author[1]{Roberto Labadie Tamayo}[%
email=rlabadiet@gmail.com,
]
\address[1]{Universidad de Oriente, Cuba}

\author[2]{Reynier Ortega Bueno}[%
email=rortega@prhlt.upv.es,
]

\address[2]{PRHLT Research Center, Universitat Politècnica de València, Valencia Spain}

%% Footnotes

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  A clear and well-documented \LaTeX{} document is presented as an
  article formatted for publication by CEUR-WS in a conference
  proceedings. Based on the ``ceurart'' document class, this article
  presents and explains many of the common variations, as well as many
  of the formatting elements an author may use in the preparation of
  the documentation of their work.   A clear and well-documented \LaTeX{} document is presented as an  article formatted for publication by CEUR-WS in a conference   proceedings. Based on the ``ceurart'' document class, this article
  presents and explains many of the common variations, as well as many
  of the formatting elements an author may use in the preparation of
  the documentation of their work.  A clear and well-documented \LaTeX{} document is presented as an  article formatted for publication by CEUR-WS in a conference   proceedings. Based on the ``ceurart'' document class, this article
  presents and explains many of the common variations, as well as many
  of the formatting elements an author may use in the preparation of
  the documentation of their work.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{keywords}
  Linguistic Ensemble \sep
  Paraphrasal Criteria Augmentation \sep
  Sexism \sep
  Transformers
\end{keywords}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

In the last few years a diversity of social problems has been tackled as Natural Language Processing (NLP) tasks, resulting in the development of robust Artificial Intelligence (AI) models with considerable results. Nevertheless, each of these tasks involves particular challenges taking into account communicative devices employed by humans, which hamper for machines to understand the language determinism.
\\
Sexism is an inherent cultural phenomenon of our society which can be easily verifiable in the way people (independently of their sexual gender) express themselves. Hence social media, as a mirror of the tangible reality, suffer from this phenomenon added to a commonly observed toxicity, with written manifestations ranging from stereotyping and objectification to sexually violent positions of users. 
\\
In this biased media, where the data for developing the above-referred models is taken from, it is important to determine whether messages are sexist, to develop unbiased and detoxified software. On the other hand, many studies stating the social networks as a place where besides positive processes converge an important portion of psychological violence, argues for its relation with the emotional and physical health harming of women being target of sexist content \cite{berg2001everyday}. About the concept of ``online'' and ``offline'' persona, \cite{10.1016/j.chb.2015.06.024} conclude with certainty that ``interacting with sexist content online can indeed carry over to sexist attitudes offline''. Taking this into account, as long as a platform is able to protect its users from toxicity, it prevents its spreading and helps to stop the trivial assumption of this inherent phenomenon.\\\\	
Sexism often comes from either offensive or aggressive messages, but sometimes is masked with funny and/or friendly comments; here is where part of the task complexity comes from, since the text isolation of a contextual environment may difficult the sexism detection even for humans. Also, due to the cultural-driven perception of this phenomenon, the availability of annotated data for developing supervised methods of classification is not such a mounting quantity. Finally, sexism perception as well as any other form of verbal language, is subject to additional knowledge from the source, e.g.,  gestures, prosody features, visual content, and situational environment, which do not accompany textual information in social media.
\\
All of this represent challenging facts for sexism detection task within  NLP. Following this idea, the task EXIST: sEXism Identification in Social neTworks at IberLEF 2022 \cite{EXIST2022}, aims at computationally recognizing sexist language on messages from microblogging social media (i.e., Twitter), as well as categorizing it attending to the type of sexism from a multilingual perspective of the phenomenon and as a continuation of the first shared task EXIST@IberLEF 2021 \cite{PLN6389}.
\\
From last year competition, many models were proposed to face these tasks, the approaches mainly were supported on representations determined from state-of-the-art transformers-based Language Models (LM) \cite{vaswani2017attention,DBLP:journals/corr/abs-1810-04805,liu2019roberta,CaneteCFP2020}, specifically the top 10 ranked teams employed these arquitectures.  The best performed model proposed by \cite{https://doi.org/10.48550/arxiv.2111.04551}, assembled different trasnformer models based on BERT for English and BETO for Spanish and a multilingual variant of BERT combining the prediction of them and taking the higher standardized value in their prediction units. The system proposed by \cite{Schtz2021AutomaticSD}  employed a fine-tuned XLM-R over an extended EXIST dataset by incorporanting MeTwo  dataset \cite{9281090} and HatEval 2019 dataset \cite{basile-etal-2019-semeval}. Other models employed traditional Machine Learning models such as Logistic Regression, Support Vector Machines, Random Forest \cite{DBLP:conf/sepln/KumarPP21,unedunbased}.\\
Other works have explored individually the detection of stereotyped hate speech spreading taking into account the misogynist form  of communication \cite{basile-etal-2019-semeval,bevendorff:2021a} as well as the masked form of this kind of targeted speech with funny messages \cite{PLN6394,43a63aada26e44b0b5bb110cb1934c04}. In those contexts the systems best ranked also were aligned with the use of sota language models yielding robust models in front of these tasks.\\
In this working notes we introduce our architecture for participating in the EXIST task at IberLEF 2022 and we study how augmenting the prediction criteria for models ensembles in two different ways influence that kind of approaches. Also we evaluate the expansion of learning examples for our models by introducing data coming from different sources versus expanding them by paraphrasing the provided data in competition. The source code of our approach is available on GitHub \footnote{\url{https://github.com/labadier/EXIST}}.

The paper is organized as follows: in Section 2 we briefly introduce a description of the tasks and datasets employed. Section 3 presents the system’s architecture and provides details about its modules and methodologies. Section 4 describes the experiments and the achieved results. Finally, we present our
conclusions and provide some directions that we plan to explore in future work.

\section{Task and Datasets Description }

At EXIST 2022 as in 2021 edition, were proposed two subtasks, the first one aiming to determine whether a tweet contains sexist expressions or behaviors. Whereas the second task aimed to determine, in such tweets labeled as sexist, the category of the message according to the type of sexism, i.e., the different facets of women that are undermined: Ideological and inequality, stereotyping and dominance, objectification, sexual violence and mysogyny and no-sexual violence.
\\
Organizers proposed the same training dataset from 2021 edition with roughly 6980 examples, whose data distribution for the first subtask was balanced among positive and negative classes. A different situation with respect to the labeling balance for second subtask is observed, where objectification and sexual-violence categories where softly underrepresented.
\\
We also introduced examples from other datasets where sexism is disguised with humor, or it is employed through an ambivalent way by glorification  of traditionally feminine behaviors or demonizing “unladylike” behavior in media coverage with a hateful language.

\subsection{MAMI Dataset}

The task Multimedia Automatic Misogyny Identification (MAMI) \cite{task5} consists in the identification of misogynous memes, taking advantage of both text and images available as source of information. From this data we took the text transcription and its annotations for training models in both, first and second subtaks, since the explored categories were almost aligned with the ones evaluated in EXIST. We mapped the annotations from MAMI dataset to EXIST categories as follows; from \textit{shaming}  to \textit{ideological-inequality} and \textit{misogyny-non-sexual-violence}, from \textit{stereotype} to \textit{stereotyping-dominance}.


\subsection{HaHa Dataset}

In HAHA@IberLEF2021: Humor Analysis based on Human Annotation \cite{PLN6394}, was proposed a dateset composed by examples of tweets written in Spanish, annotated regarding the presence of humor, funniness score prediction, the humor mechanism employed (i.e., parody, stereotype, etc.), and the humor target, i.e.,  for a humorous tweet, the target of the joke from a set of classes such as racist jokes, sexist jokes, etc  (what it is making fun of). 
Here, as examples of sexist tweets, we took all of those labeled as humorous and whose target was related to sexist jokes, afterwards we mapped their annotated targets into their corresponding categories from EXIST task taking into account the mechanism employed (e.g., stereotyping, insults, etc.).

\subsection{Hahackathon Dataset}
	
	For the HaHackathon@SemEval2021: Detecting and Rating Humor and Offense, the dataset provided by the organizers contains English tweets annotated with the presence of humor as well as the humor rating, controversiality and offensiveness rating of the messages. From this dataset, considering the overlapping between humor and offensiveness, we used as positive examples of sexism, and just for subtask 1, those tweets containing popular expressions and terms commonly used to underestimate the role of women in our society and/or spread hate on them and were annotated with values of offensiveness rating greater or equal than 1.0.
	
	
\section{Systems Overview} \label{ovv}

	We explored the performance of two main methods for facing sexism detection and classification tasks, both of them consisting in increasing the data points resulting from mixing the HaHa, Hahackathon, MAMI and EXIST data ($\mathcal{D}-dataset$).  In the first one we augmented the points	before classification (i.e., in the training stage), via back-translation, taking as pivot-languages different tongs (i). The second strategy relied on augmenting the points at classification stage by assembling the predictions from multiple language models after translating the input message into their respective language (ii).
	\\
	These methods were based on transformer architectures from the HuggingFace Transformers library\footnote{https://huggingface.co/transformers}, from there, we simply finetuned these models over the respective approach dataset taking as target task a multitask perspective involving subtasks 1 and 2. Over this simple paradigm we study their performance.
	
	\subsection{Data Augmentation}
	
	For augmenting data we selected six languages (i.e., es, en, fr, de, pt, it). For approach (i) we paraphrased the tweets translating all the messages into those pivot-languages and then back into Spanish and English, in such a way that for every examples in the original a dataset (i.e., the one composed by all shared tasks data) we introduce 5 new points.
	For (ii) we keep the points from  first step in order to have data for fine-tuning models pretrained in those languages. To carry out, every step the library googletrans\footnote{https://pypi.org/project/googletrans/} was employed.
	
	\subsection{Transformers Fine-tuning}
	
	Transformer models with a RoBERTa-based \cite{liu2019roberta} pre-training procedure over a BERT-base \cite{DBLP:journals/corr/abs-1810-04805} configuration were fine-tuned for each language. Specifically, for English language we used BERTweet \cite{bertweet}, for Spanish BETO pretrained on sentiment analysis data \cite{perez2021pysentimiento}, for French FlauBERT \cite{le2020flaubert}, for Portuguese BERTimbau \cite{souza2020bertimbau}, for Italian the model Italian-BERT \cite{stefan_schweter_2020_4263142} and finally for German a BERT model pretrained on German language texts \cite{guhr-EtAl:2020:LREC}.
	\\
	For all of them we employed as in \cite{tamayo:2021} a gradual unfreezing-discriminative fashion, setting a different learning rate for each encoder layer. Each example  was labeled according the annotation obtained in the original dataset aggregation $\mathcal{D}-dataset$, masking the error propagation from units corresponding to subtask 2 in those cases were information regarding to this was not available. 
	
	\subsection{Points Prediction Ensembling}
	
	As methods for ensembling predictions made by language models we explored (i) a simple majority voting strategy and (ii) making a vector representation of the examples, composed by the softmax layer output of each model and fed it into a parsimonious model. We hypothesized that these expansion of classification criteria, given by the outputs of different models with a pretraining approached over different data domains would introduce a denoising effect into our system.\\
	Following this line, we also explored prediction augmentation, i.e., employing the back-translation technique just at testing time and making predictions for different paraphrases of the input text in order to combine them, since in front textual information with same semantic kernel, language models must preserve their perception of sexist content.
	
	\section{Experimental Results}
	
	In this section we describe the conducted experiments for evaluating the performance of our systems on the training dataset employing a 5-fold cross-validation fashion. For that, we employed accuracy (Acc) metric. 
	\\
	Since both the strategies relied fundamentally in the use of transformer models, at first we tune them adding an intermediate linear layer ReLU-activated between the encoder module and the softmax classification layer optimizing their parameters with the RMSprop algorithm \cite{hinton2012lecture} employing as we mention an increasing learning rate from the shallower leayers to the deeper ones with a 0.01 rate from an initial learning rate from the set (1e-5, 2e-5, 3e-5, 5e-5).
	\\\\
	Regarding the use or not of external data (\textit{external}) or no (\textit{provided})  over a multitask approach and considering the best combination of 64 units for the intermediate hidden layer, batch size of 64 examples, initial learning rate of 2e-5, the \tablename ~\ref{data-augm} shows the results of each language model.
	\begin{table}[thb!]
		\begin{center} 		
			\caption{Employing External Data for each Language Model. acc(task1/task2)}	
			\begin{tabular}{lcccccc} 
				\hline	\multirow{2}{*}{Strategy}&\multicolumn{6}{c}{Language}\\
				\cline{2-7}
				&\textit{EN}&\textit{ES}&\textit{DE}&\textit{FR}&\textit{PT}&\textit{IT}\\
				\hline
				external &0.886/0.756&0.887/0.794&0.751/0.552&0.778/0.585&0.773/0.583&0.782/0.590\\
				provided&0.84/0.702&0.889/0.691&0.701/0.580&0.753/0.574&0.760/0.551&0.413/0.611\\
				\hline
			\end{tabular}
			\label{data-augm}		
		\end{center}
	\end{table}		 	
	\\
	As we can observe introducing different data from different domains improved the performance in almost all the scenarios. Nevertheless for some languages the models were not able to yield good enough results compared to English and Spanish pretrained LM.  We hypothesize that this imbalance in the performance is due to the domain of the data that the models were pretrained on.
	\\	 \\
	We also study if by making the models to learn from both substasks at the same time (\textit{mtl}), they gain enough lingual information to improve the performance or it simply introduce noise to the learning process w.r.t. a single task learning approach (\textit{stl}). Resulting the best strategy for our system combining those both task in just one learning flow as we can se in \tablename~\ref{multi-single}.
		\begin{table}[thb!]
		\begin{center} 		
			\caption{Employing External Data for each Language Model. acc(task1/task2)}	
			\begin{tabular}{lcccccc} 
				\hline	\multirow{2}{*}{Strategy}&\multicolumn{6}{c}{Language}\\
				\cline{2-7}
				&\textit{EN}&\textit{ES}&\textit{DE}&\textit{FR}&\textit{PT}&\textit{IT}\\
				\hline
					\textit{\textbf{mtl}} &0.886/0.756&0.887/0.794&0.751/0.552&0.778/0.585&0.773/0.583&0.782/0.590\\
			\textit{	stl}&0.891/0.723&0.832/0.690&0.738/0.594&0.778/0.580&0.751/0.524&0.656/0.599\\
				\hline
			\end{tabular}
			\label{multi-single}		
		\end{center}
	\end{table}		 		
		\\
 	The evaluation on the test set from EXIST2021 of the first approach from Section \ref{ovv}, i.e., augmenting the data-points at training stage via-backtranslation; shows a high variance with respect to the dev-sets resulting from cross-validating on the training set, as we can see in \tablename~\ref{points-augm}. 
		\begin{table}[thb!]
			\begin{center} 		
				\caption{Results on EXIST2021 test-set of Augmenting Data-points on Training Stage}	
				\begin{tabular}{lccc} 
					\hline	\multirow{2}{*}{Strategy}&\multicolumn{2}{c}{Language}&\multirow{2}{*}{Overall}\\
					\cline{2-3}
					&\textit{EN}&\textit{ES}&\\
					\hline
					subtask 1 &0.767&0.753&0.76\\
					subtask 2&0.622&0.608&0.615\\
					\hline
				\end{tabular}
				\label{points-augm}
			\end{center}
		\end{table}		 	
	These unbiased results of the models suggest that even when data explored by the models was augmented, their generalization capability was not enough to consider the diverse ways that sexist comments can be expressed.\\\\
	For the second approach, i.e., ensembling LMs predictions by majority voting,  we tried the different combinations of models predictions, based on the unbalanced individual accuracy observed from \tablename~\ref{multi-single} and \tablename~\ref{data-augm} , without removing, of course, the original languages of the data, i.e., English and Spanish. From these combinations the best performed was the ensemble considering all the languages but German with a clear improvement w.r.t. the first analyzed strategy.
	\begin{table}[thb!]
		\begin{center} 		
			\caption{Results on EXIST2021 test-set of Ensembling Multiple Language Models}	
			\begin{tabular}{lccc} 
				\hline	\multirow{2}{*}{Strategy}&\multicolumn{2}{c}{Language}&\multirow{2}{*}{Overall}\\
				\cline{2-3}
				&\textit{EN}&\textit{ES}&\\
				\hline
				subtask 1 & 0.779& 0.798& 0.789\\
				subtask 2& 0.638& 0.668& 0.653\\
				\hline
			\end{tabular}
			\label{ensemble}
		\end{center}
	\end{table}		 	
	\\
	As an alternative for this approach, we test the combination of the output from the softmax layer of each of these models which later were employed to train a Support Vector Machines to avoid the selection of an specific combination of languages. Nevertheless the performance get worst, i.e., an overall accuracy of  0.771 for subtask 1 and 0.591 for subtask 2. 
	\\
	Finally as another way to increasing the data-points at the evaluation stage, we explored back-translation of the input message employing as pivot each of the studied  languages, in such a way we again have 5 new contrastive points for major voting one example by employing the same model, i.e., one for English and one for Spanish, resulting again in a worst overall performance of 0.766 for subtask 1 and 0.651 for subtask 2.
	\\\\
	Considering these results and in (ii) we simply take the input sequence into different
	domains where its corresponding model is trained on, and in (i) we paraphrase the input sequence by making the semantic information flowing towards a pivot language and afterwards back to the original language we may hypothesize:
	\begin{itemize}
			\item[a.] For approach (i) some phrases can be contrastive among their different translations in terms of the sexism perception of the evaluated Language Model. This possibly caused when the text flowed from the pivot language back to the original language the performance of the prediction decreased.
			\item [b.] Taking the texts into different domains and employing specific LM for each domain helped smoothing the noise produced for an individual model in this sexism detection task.
	\end{itemize} 
With respect to the best ranked team in the last year competition our approach
introduces new data from sexism-related task and extends the use of a wider range of
languages, which allow us to outperform their result of 0.780 of acc and makes our system functional not just for english or spanish languages, but also with (fr, de, pt, it). We evaluated denoising the predictions not just by ensembling multiple language models but introducing back-translation on testing-time, which allow us to conclude the existence of vanishing in te sexism perception by the language model when the phrases are taken into another domain where the LM are not trained in.
\\\\
Regarding official results on this edition of EXIST task we were allowed to make three submissions,the first one employing the majority voting ensemble strategy, achieving  an accuracy of 75,80\% and and  65,69\% for first and second subtasks respectively and as secondary metric evaluated by the organizers 0,7559	and 0.4635 points of  macro-average F-measure. As second submission, the denoised predictions obtained from back-translating at testing stage, obtaining an accuraccy of 74,57\% and 63,71\% for subtasks 1 and 2 respectively whereas for macro-average F-measure 0,7426 and 0,4325 points for each subtask respectively. Finally our worst performed submission considered the softmax output analysis through an SVM, yielding in terms of accuracy 49,05\% in subtask 1 and 31,10\% in subtask 2, and  macro-average F-measures of  0,4872 and 0,1508.

\section{Conclusion and Future Works}

In this paper we describe our system and workflow for facing the task EXIST: sEXism Identification in Social neTworks at IberLEF 2022, consisting of given a message, determining whether it contains sexist undertones as well as the kind of sexism. We addressed this task at the time we study the impact of augmenting the data-points at training stage employing back-translation technique or at testing stage by incorporating different state of the art Languages Models to make individual predictions for a final voting decision when classifying.
\\
We achieved the best performance of our system in competition by combining the prediction of Language Models pretrained on English, Spanish, Portuguese, Italian and French, which were fed with a translated version of the input message into their respective tong reporting an accuraccy of 75,8\% and  65,69\% for first and second subtasks respectively.\\
************ hablation wrt the objetive of the paper and results obtained.
\begin{acknowledgments}
	
\end{acknowledgments}

%%
%% Define the bibliography file to be used
\bibliography{bibs}

\end{document}

%%
%% End of file
